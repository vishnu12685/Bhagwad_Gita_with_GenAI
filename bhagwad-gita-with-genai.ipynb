{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11462559,"sourceType":"datasetVersion","datasetId":7182703}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re  \n\ndef load_text(file_path):  \n    with open(file_path, 'r', encoding='utf-8') as file:  \n        text = file.read()  \n    return text  \n\ndef preprocess_text(text):  \n    # Remove non-English characters and symbols  \n    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only English letters and whitespace  \n    text = ' '.join(text.split())  # Remove extra spaces  \n    text = text.lower()  # Convert to lowercase  \n    return text  \n\ndef split_into_chunks(text, chunk_size=400):  # Reduced chunk size  \n    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]  \n    return chunks  \n\n# Load and preprocess the text  \nfile_path = \"/kaggle/input/bhagwat-gita/The Bhagavad Gita.txt\"  # Replace with your file path  \ntext = load_text(file_path)  \ncleaned_text = preprocess_text(text)  \nchunks = split_into_chunks(cleaned_text)  \n\n# Save preprocessed chunks (optional)  \nwith open('preprocessed_chunks.txt', 'w', encoding='utf-8') as file:  \n    for chunk in chunks:  \n        file.write(chunk + '\\n')  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:09:20.620843Z","iopub.execute_input":"2025-04-18T14:09:20.621208Z","iopub.status.idle":"2025-04-18T14:09:20.689556Z","shell.execute_reply.started":"2025-04-18T14:09:20.621173Z","shell.execute_reply":"2025-04-18T14:09:20.688772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U langchain-community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:09:20.691165Z","iopub.execute_input":"2025-04-18T14:09:20.691508Z","iopub.status.idle":"2025-04-18T14:09:24.780201Z","shell.execute_reply.started":"2025-04-18T14:09:20.691481Z","shell.execute_reply":"2025-04-18T14:09:24.779158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Generate embeddings and create a FAISS vector database  \nfrom sentence_transformers import SentenceTransformer\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom typing import List, Optional\n\ndef create_vector_database(\n    chunks: List[str], \n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',\n    save_path: Optional[str] = None\n) -> FAISS:\n    \"\"\"\n    Creates a FAISS vector database from text chunks using HuggingFace embeddings.\n    \n    Args:\n        chunks (List[str]): List of text chunks to be embedded\n        model_name (str): Name of the sentence transformer model to use\n        save_path (Optional[str]): Path to save the vector database. If None, database won't be saved\n    \n    Returns:\n        FAISS: A FAISS vector database containing the embeddings\n        \n    Raises:\n        ValueError: If chunks list is empty\n    \"\"\"\n    if not chunks:\n        raise ValueError(\"Chunks list cannot be empty\")\n    \n    # Initialize the embedding model\n    embedding_function = HuggingFaceEmbeddings(\n        model_name=model_name,\n        model_kwargs={'device': 'cpu'}\n    )\n    \n    # Create FAISS vector database\n    vector_db = FAISS.from_texts(\n        texts=chunks,\n        embedding=embedding_function\n    )\n    \n    # Save the database if path is provided\n    if save_path:\n        vector_db.save_local(save_path)\n    \n    return vector_db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:09:24.781880Z","iopub.execute_input":"2025-04-18T14:09:24.782268Z","iopub.status.idle":"2025-04-18T14:09:24.790378Z","shell.execute_reply.started":"2025-04-18T14:09:24.782229Z","shell.execute_reply":"2025-04-18T14:09:24.789345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install faiss-cpu\n!pip install huggingface_hub[hf_xet]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:13:11.690945Z","iopub.execute_input":"2025-04-18T14:13:11.691320Z","iopub.status.idle":"2025-04-18T14:13:23.886183Z","shell.execute_reply.started":"2025-04-18T14:13:11.691295Z","shell.execute_reply":"2025-04-18T14:13:23.884971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer  \nfrom langchain.vectorstores import FAISS  \n\n# Generate embeddings with truncation  \nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')  \nembeddings = model.encode(chunks, truncate=True)  # Truncate excess tokens  \n\n# Create and save the vector database  \nvector_db = FAISS.from_embeddings(  \n    text_embeddings=list(zip(chunks, embeddings)),  \n    embedding=embeddings[0]  \n)  \nvector_db.save_local(\"faiss_vector_db\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:13:38.592467Z","iopub.execute_input":"2025-04-18T14:13:38.592850Z","iopub.status.idle":"2025-04-18T14:14:33.242012Z","shell.execute_reply.started":"2025-04-18T14:13:38.592818Z","shell.execute_reply":"2025-04-18T14:14:33.240934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def retrieve_relevant_chunks(query, vector_db, model, top_k=3):  \n    query_embedding = model.encode([query])  \n    relevant_chunks = vector_db.similarity_search_by_vector(query_embedding[0], k=top_k)  \n    return relevant_chunks  \n\n# Example retrieval  \nquery = \"What is the concept of Karma in the Bhagavad Gita?\"  \nrelevant_chunks = retrieve_relevant_chunks(query, vector_db, model)  \nfor i, chunk in enumerate(relevant_chunks):  \n    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:15:52.619720Z","iopub.execute_input":"2025-04-18T14:15:52.620074Z","iopub.status.idle":"2025-04-18T14:15:52.667705Z","shell.execute_reply.started":"2025-04-18T14:15:52.620049Z","shell.execute_reply":"2025-04-18T14:15:52.666727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install google-generativeai langchain-google-genai sentence-transformers faiss-cpu  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:17:45.864671Z","iopub.execute_input":"2025-04-18T14:17:45.864984Z","iopub.status.idle":"2025-04-18T14:19:32.415335Z","shell.execute_reply.started":"2025-04-18T14:17:45.864963Z","shell.execute_reply":"2025-04-18T14:19:32.413524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install google-generativeai sentence-transformers scikit-learn numpy  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:26:15.915903Z","iopub.execute_input":"2025-04-18T14:26:15.916324Z","iopub.status.idle":"2025-04-18T14:26:20.318937Z","shell.execute_reply.started":"2025-04-18T14:26:15.916267Z","shell.execute_reply":"2025-04-18T14:26:20.317708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import google.generativeai as genai  \nfrom sentence_transformers import SentenceTransformer  \nimport numpy as np  \nimport os  \nfrom sklearn.metrics.pairwise import cosine_similarity  \n\n# 1. Configure Google Gemini  \nos.environ['GOOGLE_API_KEY'] = 'AIzaSyDx7yJCneYXXTXP1I4ed03QN0saGwYRjHI'  # Replace with your actual API key  \ngenai.configure(api_key=os.environ['GOOGLE_API_KEY'])  \n\n# 2. Initialize embedding model (no LangChain)  \nembedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')  \n\n# 3. Load your documents (replace with your actual document loading logic)  \ndocuments = [  \n    \"The Bhagavad Gita teaches...\",  # Your document 1  \n    \"Karma yoga emphasizes...\",      # Your document 2  \n     \n]  \n\n# 4. Pre-compute embeddings  \ndocument_embeddings = embedding_model.encode(documents)  \n\ndef retrieve_relevant_documents(query, k=3):  \n    \"\"\"Semantic search without FAISS\"\"\"  \n    query_embedding = embedding_model.encode(query)  \n    similarities = cosine_similarity([query_embedding], document_embeddings)[0]  \n    most_relevant = np.argsort(similarities)[-k:][::-1]  \n    return [documents[i] for i in most_relevant]  \n\ndef get_answer(question):  \n    try:  \n        # Retrieve context  \n        context = \"\\n\".join(retrieve_relevant_documents(question))  \n        \n        # Generate answer using pure Gemini API  \n        model = genai.GenerativeModel('gemini-1.5-pro-latest')  \n        response = model.generate_content(  \n            f\"\"\"Answer this question about the Bhagavad Gita:  \n            \n            Question: {question}  \n            \n            Context: {context}  \n            \n            Provide a detailed answer with relevant verses:\"\"\"  \n        )  \n        return response.text  \n    except Exception as e:  \n        return f\"Error: {str(e)}\"  \n\n# Example usage  \nif __name__ == \"__main__\":  \n    question = \"Explain the concept of Dharma in the Bhagavad Gita\"  \n    answer = get_answer(question)  \n    print(\"Question:\", question)  \n    print(\"Answer:\", answer)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:27:53.333018Z","iopub.execute_input":"2025-04-18T14:27:53.333475Z","iopub.status.idle":"2025-04-18T14:28:11.146618Z","shell.execute_reply.started":"2025-04-18T14:27:53.333398Z","shell.execute_reply":"2025-04-18T14:28:11.145532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    question = \"What is concept of Bhaktiyog and how it is different from gyanyog\"  \n    answer = get_answer(question)  \n    print(\"Question:\", question)  \n    print(\"Answer:\", answer) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:49:49.119308Z","iopub.execute_input":"2025-04-18T14:49:49.119681Z","iopub.status.idle":"2025-04-18T14:50:11.490221Z","shell.execute_reply.started":"2025-04-18T14:49:49.119656Z","shell.execute_reply":"2025-04-18T14:50:11.489295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    question = \"Explain the concept of krishna as Superior of all and give examples to support it\"  \n    answer = get_answer(question)  \n    print(\"Question:\", question)  \n    print(\"Answer:\", answer) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:52:43.186879Z","iopub.execute_input":"2025-04-18T14:52:43.187320Z","iopub.status.idle":"2025-04-18T14:53:02.575025Z","shell.execute_reply.started":"2025-04-18T14:52:43.187293Z","shell.execute_reply":"2025-04-18T14:53:02.573852Z"}},"outputs":[],"execution_count":null}]}